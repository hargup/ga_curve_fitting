#start_imports
from random import Random
from time import time
from math import pi
from inspyred import ec
from inspyred.ec import terminators
import numpy as np
from matplotlib import pyplot
#end_imports

# The problem in concern is the of fitting a of
# appropriate order among a lis of 2D points.

# X = np.array([0, 0])
# Y = np.array([0, 0])
# N = 2


def evaluate_distance(line, point):
    """
    Evaluate the perpendicular distance between a line a point.
    The line is given in parameric form as of x/a + y/b = 1
    """
    assert line[1] <= pi and line[1] >= - pi

    (x, y) = point
    (r, theta) = line
    return r - (x*np.cos(theta) + y*np.sin(theta))


def test_evaluate_distance():
    assert evaluate_distance((1, pi/4), (0, 0)) == 1


def evaluate_error(lines, points):
    """
    Lines: [(r1, theta1), ...]
    """

    # XXX: for large values 10^5 of r in a line
    # the error becomes large enough to cross the limits of
    # floating points number, in that case the error is shown to zero.

    # X = args.get('X')
    # Y = args.get('Y')

    (X, Y) = points

    assert len(X) == len(Y)

    dists = [X*np.cos(theta) + Y*np.sin(theta) - r
             for (r, theta) in lines]

    sds = [dist.std() for dist in dists]

    errors = []

    for (dist, sd) in zip(dists, sds):
        dist[(dist > 2*sd) | (dist < -2*sd)] = 0
        ssd = dist*dist/len(X)
        errors.append(ssd.sum())

    return np.array(errors)


def evualuate_plain_ssd_error(lines, points):
    (X, Y) = points

    assert len(X) == len(Y)

    dists = [X*np.cos(theta) + Y*np.sin(theta) - r
             for (r, theta) in lines]

    sds = [dist.std() for dist in dists]

    errors = []

    for (dist, sd) in zip(dists, sds):
        ssd = dist*dist/len(X)
        errors.append(ssd.sum())

    return np.array(errors)


# def test_evaluate_error():
#     assert evaluate_error((0, 0), [0, 0, 0, 0], [0, 0, 0, 0]) == 0


def generate_line(boundsX, boundsY):
    # XXX: look the bounds have not been set properly due to lazyness
    p = (np.random.uniform(boundsX[0]/2, boundsX[1]/2),
         np.random.uniform(0, pi))
    if not 0 <= p[1] <= pi:
        print "p: ", p
        assert 1 == 0
    return p


def generate_gaussian_noise(line, sd, boundsX, boundsY, n=100):
    # The noise is generated by two random variables, on is the random position
    # of the point w.r.t line and second is the distance
    (r, theta) = line

    low_x = boundsX[0]
    high_x = boundsX[1]

    low_y = boundsY[0]
    high_y = boundsY[1]

    # line_X = np.random.uniform(low=low_x, high=high_x, size=n)
    #
    # line_Y = (r - line_X*np.cos(theta))/np.sin(theta)
    #
    # distx = np.random.normal(scale=sd, size=n)
    # disty = np.random.normal(scale=sd, size=n)

    X = np.random.uniform(low=low_x, high=high_x, size=n)

    Y = (r - X*np.cos(theta))/np.sin(theta) + np.random.normal(0,
                                                               sd*np.sqrt(2),
                                                               n)

    # Y = (r - X*np.cos(theta))/np.sin(theta)

    # X = line_X + dist*np.sin(theta)
    # Y = line_Y - dist*np.cos(theta)

    # X = line_X + distx
    # Y = line_Y + disty

    # X = line_X
    # Y = np.random.uniform(low=low_y, high=high_y, size=n)

    mask_x = (X > low_x) & (X < high_x)
    mask_y = (Y > low_y) & (Y < high_y)

    mask = mask_x & mask_y

    X = X[mask]
    Y = Y[mask]

    return (X, Y)


def generate_uniform_noise(line, sd, boundsX, boundsY, n=100):

    low_x = boundsX[0]
    high_x = boundsX[1]

    low_y = boundsY[0]
    high_y = boundsY[1]

    X = np.random.uniform(low=low_x, high=high_x, size=n)
    Y = np.random.uniform(low=low_y, high=high_y, size=n)

    return (X, Y)


def generate_points(line, sd, boundsX, boundsY, n=100, p=0.01):
    # p: probablity of the point comming from uniform noise
    # XXX: floating point rounding off error might make the final lengths
    # of X and Y
    # different than n

    (X_norm, Y_norm) = generate_gaussian_noise(line=line, sd=sd,
                                               boundsX=boundsX,
                                               boundsY=boundsY,
                                               n=n*(1-p))
    (X_uni, Y_uni) = generate_uniform_noise(line=line, sd=sd, boundsX=boundsX,
                                            boundsY=boundsY, n=n*p)

    X = np.append(X_norm, X_uni)
    Y = np.append(Y_norm, Y_uni)

    return (X, Y)


def test():
    test_evaluate_distance()
    # test_evaluate_error()


def plot_graph(line):
    # (X, Y) = points
    (r, theta) = line

    x0 = boundsX[0]
    y0 = (r - x0*np.cos(theta))/np.sin(theta)

    x1 = boundsX[1]
    y1 = (r - x1*np.cos(theta))/np.sin(theta)

    pyplot.plot(X, Y, "o")
    pyplot.plot([x0, x1], [y0, y1], 'r-')

    pyplot.axis([boundsX[0], boundsX[1], boundsY[0], boundsY[1]])
    pyplot.show()

# test the functions #
test()

n = 1000
boundsX = (-80, 80)
boundsY = (-80, 80)
p = 0.1

L = generate_line(boundsX, boundsY)
(X, Y) = generate_points(line=L, sd=boundsX[1]/4, boundsX=boundsX,
                         boundsY=boundsY, p=p, n=n)
print("Our Method", evaluate_error([L], (X, Y)))
print("SSD: ", evualuate_plain_ssd_error([L], (X, Y)))

# plot_graph(L)


pop_size = 100
k = pop_size/3
no_gen = 20


def generate_population(size, points, boundsX, boundsY):
    (X, Y) = points
    lines = [generate_line(boundsX, boundsY) for i in xrange(size)]
    return np.array(lines)


def crossover(p1, p2):
    c = ((p1[0] + p2[0])/2, (p1[1] + p2[1])/2)
    if not 0 <= c[1] <= pi:
        print p1, p2
        assert 0 <= c[1] <= pi
    return c


def kill_n_reproduce(pop, k, points, noise=True, factor=2):
    (X, Y) = points
    fitness = evaluate_error(pop, (X, Y))
    pos = fitness.argsort()
    pop = pop[pos]

    pop[k:] = 0

    pop[k:] = [crossover(pop[np.random.randint(0, k)],
                         pop[np.random.randint(0, k)])
               for i in range(len(pop) - k)]

    # if noise is True:
        # 8:21pm XXX: I have still not resolved the issue with larger
        # s.d
        # The '+' is having a strange effect. If I change the sign from '+'
        # to '-' the solution doesn't diverge to very large values of r
        # Ideally the this should be independent of the sign of the
        # If I add or subtract gaussian function is symettric.
        # Don't know WTF is going on

        # The fuck: the value of r doesn't diverge even if I do this
        # pop = np.array([(r + 0.001, theta)
        #                 for (r, theta) in pop])
        # Maybe I don't understand the usage of np.random.gaussian
        # TODO: read the numpy doc
        # FUCKKK I got it. I was adding the gaussian noise around r to r. So
        # for every iteration it doubled, thats why even the theta reached pretty high values

        # sd = np.sqrt(fitness.std())
        # if sd == 0:
        #     print fitness
        # # pop = np.array([(np.random.normal(r, np.sqrt(var)/factor),
        # #                  np.random.normal(theta, pi/32))
        # #                 for ((r, theta), var) in zip(pop, fitness)])
        # pop = np.array([(np.random.normal(r, sd/factor),
        #                  np.random.normal(theta, pi/32))
        #                 for (r, theta) in pop])

        # Oh Yeah!! adding noise is working beautifully, if I take the factor as
        # 1 the good solutions changes too much to converge, if I don't add noise the
        # solution gets struck at some point, but if I add enough noise the solution converges.
        # Sometimes the population takes a lower value but then is stirred enough to increase in value
        # Maybe increasing K will help

    # pop[-k:] = generate_population(k, points, boundsX, boundsY)
    return pop


def simulate(noise=True, factor=2):
    pop = generate_population(pop_size, (X, Y), boundsX, boundsY)
    pop.sort()
    print("After gen ", 0, " the fitness is: ",
          evaluate_error([pop[0]], (X, Y)))
    print("And the solution is: ", pop[0])

    for i in xrange(1, no_gen + 1):
        pop = kill_n_reproduce(pop, k, (X, Y), noise=noise, factor=factor)
        pop.sort()
        print("After gen ", i, " the fitness is: ",
              evaluate_error([pop[0]], (X, Y)))
        print("And the solution is: ", pop[0])
        print()
    return pop


def linear_regression(X, Y):
    x = X.reshape(len(X), 1)
    x_ = x.transpose()

    y = Y.reshape(len(Y), 1)
    y_ = y.transpose()

    m = np.linalg.inv(np.dot(x_, x))
    m_ = np.dot(m, x_)
    b = np.dot(m_, y)
    return b

# pop = generate_population(10, (X, Y), boundsX, boundsY)
sol = simulate()

# TODO:
    # Generate independent genepools and see the results
    # Compare with the results of linear regression
    # Choose a bette name
    # Read about linear regression and other methods

# The methods doesn't seems to be particularly novel. The thing is
# if you take a lot of random solutions choose the best out of them the chances are that
# the best one of the random solutions will be good enough. Seeing the results I couldn't make out
# if it is better than random.
#
# OOh nope, it is doing well, pretty well indeed, the first generation did so well on the initial
# random input that the further generations couldn't improve upon them.

# Because the higher generations doesn't help much it might turn out better to take multiple independent genepools
# and reducing the number of generations. Then I might define some rules for the interaction of these gene pools
# Because the error stops decreasing after only a few generations adding random solutions doesn't help. Even it
# helped in some cases it would be pure coincidence, we are the assuming that the solution space is much smaller
# than the whole space.
#
# It appears after some generations all the whole population takes the same value XXX: check it out.
# Maybe there exists many local minimas for the solution.
# I should figure out some method to make small variations in the candidate solutions,
# maybe a small gaussian noise might help



# 8:00PM 11th April
# It seems that I have run into some floating point exception
# I was trying to add guassian noise to the obtained solution with a hope that little variation in them will
# might make a bette solution out of them.
# I took the standart deviation as the s.d/2 of the fitness function.
# and for theta I took it as pi/4
# But after I did that the r and theta jumped to very large values, and error was shown as 0
#
# I checked the code for the ssd error, it doesn't produce any negative errors for points so that they some to zero.
# It is possible that the error.

# Take the means of the errors.
